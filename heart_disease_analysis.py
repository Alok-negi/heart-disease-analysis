# -*- coding: utf-8 -*-
"""heart-disease_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zncEC56Q9giJ1yHoaS2cVWx1op1PUAtf

#**Heart Disease Prediction â€“ Exploratory Data Analysis & Modeling**

### Problem Statement:
This project aims to predict the likelihood of heart disease in patients using various medical parameters. Such predictive insights can help in early intervention and healthcare planning.

### Objective:
Build and evaluate classification models to predict heart disease risk (target: `target` column).


### Business Impact:
Accurate predictions can assist medical professionals in taking preventive measures, reducing healthcare costs and saving lives.

### Load & Inspect Data

## Dataset Overview


| Feature Name | Description                                                                                                    |
| ------------ | -------------------------------------------------------------------------------------------------------------- |
| `age`        | Age of the patient in years                                                                                    |
| `sex`        | Sex of the patient (1 = male; 0 = female)                                                                      |
| `cp`         | Chest pain type (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic)              |
| `trestbps`   | Resting blood pressure (in mm Hg on admission to the hospital)                                                 |
| `chol`       | Serum cholesterol in mg/dl                                                                                     |
| `fbs`        | Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)                                                          |
| `restecg`    | Resting electrocardiographic results (0 = normal, 1 = ST-T wave abnormality, 2 = left ventricular hypertrophy) |
| `thalach`    | Maximum heart rate achieved                                                                                    |
| `exang`      | Exercise-induced angina (1 = yes; 0 = no)                                                                      |
| `oldpeak`    | ST depression induced by exercise relative to rest                                                             |
| `slope`      | Slope of the peak exercise ST segment (1 = upsloping, 2 = flat, 3 = downsloping)                               |
| `ca`         | Number of major vessels (0â€“3) colored by fluoroscopy                                                           |
| `thal`       | Thalassemia type (3 = normal, 6 = fixed defect, 7 = reversible defect)                                         |
| `num`        | Diagnosis of heart disease (0 = <50% narrowing, 1 = >50% narrowing)                                            |
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('/content/drive/MyDrive/heart-disease.csv')

data.head()

data.shape

"""### Create a copy of the original data to safely perform analysis without altering the original dataset"""

df=data.copy()

"""### Displays basic statistics (mean, std, min, max, quartiles) for numerical columns to understand their distributions.





"""

df.describe().round(2)

df.info()

"""## Data Cleaning
This dataset is mostly clean but it have some duplicates and typo errors

*   Removing Duplicates
*   checking null values
*   Removing typo errors

"""

df=df.drop_duplicates()

df.isnull().sum()

df=df[df['thal']!=0]

df.shape

"""## Renaming Columns for Better Readability"""

df.rename(columns={
    'age': 'Age',
    'sex': 'Sex',
    'cp': 'Chest_Pain_Type',
    'trestbps': 'Resting_BP',
    'chol': 'Cholesterol',
    'fbs': 'Fasting_Blood_Sugar',
    'restecg': 'Resting_ECG',
    'thalach': 'Max_Heart_Rate',
    'exang': 'Exercise_Induced_Angina',
    'oldpeak': 'ST_Depression',
    'slope': 'Slope_of_ST',
    'ca': 'Num_Major_Vessels',
    'thal': 'Thalassemia_Type',
    'num': 'Heart_Disease_Status'
}, inplace=True)

df

"""### Checking unique values for each categorical column


"""

for i in df.columns:
  print(f'Unique values in {i} column\n')
  print(df[i].unique())
  print('\n\n')

df.shape

"""## Handling Outliers"""

q1=df['ST_Depression'].quantile(0.25)
q3=df['ST_Depression'].quantile(0.75)
IQR=q3-q1
upper=q3+1.5*IQR
lower=q1-1.5*IQR
q1,q3,IQR,upper,lower

outliers=df[(df['ST_Depression']>upper) | (df['ST_Depression']<lower)]
outliers

df=df[(df['ST_Depression']<=upper) & (df['ST_Depression']>=lower)]

q1=df['Resting_BP'].quantile(0.25)
q3=df['Resting_BP'].quantile(0.75)
IQR=q3-q1
upper=q3+1.5*IQR
lower=q1-1.5*IQR
df['Resting_BP'] = df['Resting_BP'].apply(lambda x: upper if x > upper else lower if x < lower else x)
q1,q3,IQR,upper,lower

q1=df['Cholesterol'].quantile(0.25)
q3=df['Cholesterol'].quantile(0.75)
IQR=q3-q1
upper=q3+1.5*IQR
lower=q1-1.5*IQR
df['Cholesterol'] = df['Cholesterol'].apply(lambda x: upper if x > upper else lower if x < lower else x)
q1,q3,IQR,upper,lower

df.describe()

"""# ðŸ” Categorical Value Mapping and Distribution Analysis
To improve the interpretability of the dataset, we created a new DataFrame dfc by copying the original data and replacing numerical codes in categorical columns with meaningful labels. For instance:

Sex values were mapped from 0/1 to 'Female'/'Male'.

Chest_Pain_Type values from 0â€“3 were mapped to descriptive labels like 'Typical angina', 'Atypical angina', etc.

Other binary and categorical medical indicators were similarly mapped to human-readable terms.

**This transformation makes visual analysis and distribution understanding much easier**
"""

dfc=df.copy()
dfc['Sex']=dfc['Sex'].map({0:'Female',1:'Male'})
dfc['Chest_Pain_Type']=dfc['Chest_Pain_Type'].map({0:'Typical angina',1:'Atypical angina',2:'Non-anginal pain',3:'Asymptomatic'})
dfc['Fasting_Blood_Sugar']=dfc['Fasting_Blood_Sugar'].map({0:'No',1:'Yes'})
dfc['Resting_ECG']=dfc['Resting_ECG'].map({0:'Normal',1:'Abnormalities in ST segment',2:'Left Ventricular Hypertropy'})
dfc['Exercise_Induced_Angina']=dfc['Exercise_Induced_Angina'].map({0:'No',1:'Yes'})
dfc['Slope_of_ST']=dfc['Slope_of_ST'].map({0:'Upslope',1:'Flat',2:'Downslope'})
dfc['Num_Major_Vessels']=dfc['Num_Major_Vessels'].map({0:'0 visible',1:'1 visible',2:'2 visible',3:'3 visible',4:'All 4 visible'})
dfc['Thalassemia_Type']=dfc['Thalassemia_Type'].map({1:'Normal',2:'Fixed defect',3:'Reversible defect'})
dfc

for i in dfc.columns:
   plt.figure(figsize=(6,4))
   if dfc[i].dtype == 'object':
    sns.countplot(x=i, hue='target', data=dfc)
    plt.show()
    print('\n')
   elif i!='target':
    sns.histplot(data=dfc, x=i, hue='target',kde=True, multiple='dodge')
    plt.show()
    print('\n')

"""This transformation makes visual analysis and distribution understanding much easier.

After mapping, we analyzed the distribution of each categorical feature against the target variable (target, which represents presence or absence of heart disease). The following code displays, for each categorical column:



*   How many individuals of each category are diagnosed with or without heart disease
*   This helps identify potentially significant predictor





"""

for i in df.columns:
  if dfc[i].dtype == 'object':
    print(f"\nDistribution of {i} column by target class:\n")
    z = dfc.groupby(i)['target'].value_counts().unstack()
    print(z)
    print('\n\n')

"""### We converted binary features to numbers and used a heatmap to show how different variables relate. This helps spot important connections for predicting heart disease."""

num=dfc[['Age','Resting_BP','Cholesterol','Max_Heart_Rate','ST_Depression','Sex','Fasting_Blood_Sugar','Exercise_Induced_Angina']]
num['Sex']=num['Sex'].map({'Male':1,'Female':0})
num['Fasting_Blood_Sugar']=num['Fasting_Blood_Sugar'].map({'Yes':1,'No':0})
num['Exercise_Induced_Angina']=num['Exercise_Induced_Angina'].map({'Yes':1,'No':0})
num

htmap=num.corr()
sns.heatmap(htmap, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

"""#âœ… Strongest Correlations:

1 - Max_Heart_Rate and Age â†’ -0.40

    

    As age increases, the maximum heart rate achieved tends to decrease â€” which is expected due to natural decline in cardiovascular performance with age.

2 - Exercise_Induced_Angina and Max_Heart_Rate â†’ -0.38

    
    Patients who develop angina during exercise tend to have lower maximum heart rates, possibly indicating poor heart function.

3 - ST_Depression and Max_Heart_Rate â†’ -0.34

    

    Higher ST depression values are associated with lower max heart rate, suggesting that patients with reduced exercise capacity show more signs of cardiac stress.

#âœ… Moderate/Low Correlations:
   
1 - Exercise_Induced_Angina and ST_Depression â†’ 0.31


    Patients who experience angina during exercise often show higher ST depression, indicating potential signs of cardiac ischemia under stress.

2 - ST_Depression and Age â†’ -0.21


    Older patients tend to have slightly lower ST depression values, though the relationship is not very strong.

3 - Resting_BP and Age â†’ 0.29


    As age increases, resting blood pressure also tends to rise, which aligns with common medical observations about hypertension and aging.

4 - Age and Cholesterol â†’ 0.20


    Older individuals tend to have slightly higher cholesterol levels, although the relationship is not particularly strong.

### Making dummy variables for categorical columns to prepare the data for model training.
"""

dummies_df=dfc.copy()
dummies_df.describe()

dummies_df = pd.get_dummies(dummies_df, columns=['Chest_Pain_Type', 'Resting_ECG', 'Slope_of_ST', 'Num_Major_Vessels', 'Thalassemia_Type'], drop_first=True)
dummies_df.head()

dummies_df['Fasting_Blood_Sugar'] = dummies_df['Fasting_Blood_Sugar'].map({'Yes': 1, 'No': 0})
dummies_df['Exercise_Induced_Angina'] = dummies_df['Exercise_Induced_Angina'].map({'Yes': 1, 'No': 0})
dummies_df['Sex'] = dummies_df['Sex'].map({'Male': 1, 'Female': 0})

dummies_df.describe()

"""### Scaling selected numerical features using StandardScaler to normalize their values and improve model performance."""

from sklearn.preprocessing import StandardScaler

def scaling(df):
    scaler = StandardScaler()
    scale_cols = ['Age', 'Resting_BP'	,'Cholesterol', 'Max_Heart_Rate','ST_Depression']
    scaled = scaler.fit_transform(dummies_df[scale_cols])
    scaled_df = pd.DataFrame(scaled, columns=scale_cols, index=dummies_df.index)
    df_scaled = pd.concat([dummies_df.drop(columns=scale_cols), scaled_df], axis=1)
    return df_scaled
dummies_df=scaling(dummies_df)

dummies_df

"""### Spliting the data into train and test dataset"""

from sklearn.model_selection import train_test_split
train,test=np.split(dummies_df.sample(frac=1, random_state=42), [int(0.8 * len(dummies_df))])

train.shape

test.shape

train['target'].value_counts()

x_train=train.drop(columns='target')
y_train=train['target']
x_test=test.drop(columns='target')
y_test=test['target']

x_train.describe()

"""## Evaluating and comparing the accuracy of different machine learning models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

models = [
    KNeighborsClassifier(),
    RandomForestClassifier(),
    DecisionTreeClassifier(),
    SVC(),
    GaussianNB(),
    LogisticRegression()
]
for model in models:
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    print(f'Accuracy of {model}: {accuracy_score(y_test, y_pred):.4f}')

lrmodel=LogisticRegression()
lrmodel.fit(x_train,y_train)
y_pred=lrmodel.predict(x_test)
print('Acuuracy :' , accuracy_score(y_test,y_pred))
print('\nconfusion_matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nclassification_report :\n' , classification_report(y_test,y_pred))

knnmodel=KNeighborsClassifier()
knnmodel.fit(x_train,y_train)
y_pred=knnmodel.predict(x_test)
print('Accuracy :', accuracy_score(y_test,y_pred))
print('\nConfusion Matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nClassification Report :\n', classification_report(y_test,y_pred))

nbmodel=GaussianNB()
nbmodel.fit(x_train,y_train)
y_pred=nbmodel.predict(x_test)
print('Accuracy :', accuracy_score(y_test,y_pred))
print('\nConfusion Matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nClassification Report :\n', classification_report(y_test,y_pred))

svmmodel=SVC()
svmmodel.fit(x_train,y_train)
y_pred=svmmodel.predict(x_test)
print('Accuracy :', accuracy_score(y_test,y_pred))
print('\nConfusion Matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nClassification Report :\n', classification_report(y_test,y_pred))

dtreemodel=DecisionTreeClassifier()
dtreemodel.fit(x_train,y_train)
y_pred=dtreemodel.predict(x_test)
print('Accuracy :', accuracy_score(y_test,y_pred))
print('\nConfusion Matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nClassification Report :\n', classification_report(y_test,y_pred))

rfcmodel=RandomForestClassifier()
rfcmodel.fit(x_train,y_train)
y_pred=rfcmodel.predict(x_test)
print('Accuracy :', accuracy_score(y_test,y_pred))
print('\nConfusion Matrix :\n' , confusion_matrix(y_test,y_pred))
print('\nClassification Report :\n', classification_report(y_test,y_pred))

"""### The best performing models with default parameters are

### 1.   Logistic Regression (Accuracy: 0.8475)
### 2.   SVM (Accuracy: 0.8305)

### Now, we will try to improve their accuracy further by applying **hyperparameter tuning.**
"""

from sklearn.model_selection import GridSearchCV
log_reg=LogisticRegression(solver='liblinear')
hyperr={
    'C':[0.01,0.1,1,10,100],
    'penalty':['l1','l2']
}
grid=GridSearchCV(estimator=log_reg,param_grid=hyperr,cv=5,scoring='accuracy')
grid.fit(x_train,y_train)
best_model=grid.best_estimator_
y_pred=best_model.predict(x_test)
print('The best parameters are :\n',grid.best_params_)
print('\nThe best score is :',grid.best_score_)
print("\nTest Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

svm=SVC()
para = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto', 0.01, 0.1, 1]
}
grid=GridSearchCV(estimator=svm,param_grid=para,cv=5,scoring='accuracy')
grid.fit(x_train, y_train)
best_model=grid.best_estimator_
y_pred=best_model.predict(x_test)
print('The best parameters are :\n',grid.best_params_)
print('\nThe best score is :',grid.best_score_)
print("\nTest Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""# After applying hyperparameter tuning, the Logistic Regression modelâ€™s accuracy improved significantly, reaching 85%"""

